{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rdflib import Graph, Namespace\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# Base data path\n",
    "data_path = './data/ex51_new_2024-07-27'\n",
    "\n",
    "# Function to join paths and create directories if they don't exist\n",
    "def create_and_get_path(*args):\n",
    "    path = os.path.join(*args)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# Input paths\n",
    "STUDY_TTL_PATH = create_and_get_path(data_path, 'input', 'instances.ttl') # contains the actual study data\n",
    "VIOLATION_REPORT_TTL_PATH = create_and_get_path(data_path, 'input', 'violations.ttl') # contains the shacl violation test case result\n",
    "OMICS_MODEL_TTL_PATH = create_and_get_path(data_path, 'input', 'omics_model.ttl') # contains the ontology\n",
    "\n",
    "# Output paths\n",
    "VIOLATION_LIST_FILE = create_and_get_path(data_path, 'output', 'json', 'violation_list.json') # path to store the violation list\n",
    "STUDY_CSV_FILE = create_and_get_path(data_path, 'output', 'csv', 'study.csv') # path to store the tabularized study csv file\n",
    "OMICS_MODEL_UNION_VIOLATION_EXEMPLAR_TTL_PATH = create_and_get_path(data_path, 'output', 'ttl', 'omics_model_union_violation_exemplar.ttl') # contains the union of the ontology and the violation exemplars\n",
    "EXEMPLAR_EDGE_COUNT_DICT_PATH = create_and_get_path(data_path, 'output', 'json', 'exemplar_edge_count_dict.json') # contains the nested dict of exemplar edge counts\n",
    "FOCUS_NODE_EXEMPLAR_DICT_PATH = create_and_get_path(data_path, 'output', 'json', 'focus_node_exemplar_dict.json') # contains the dict of exemplars per focus node\n",
    "EXEMPLAR_FOCUS_NODE_DICT_PATH = create_and_get_path(data_path, 'output', 'json', 'exemplar_focus_node_dict.json') # contains the dict of focus nodes per exemplar\n",
    "VIOLATION_EXEMPLAR_DICT_PATH = create_and_get_path(data_path, 'output', 'json', 'violation_exemplar_dict.json') # contains the dict of exemplars and their counts per violation\n",
    "\n",
    "# define prefixes and corresponding namespaces\n",
    "prefixes = {\n",
    "    \"sh\": \"http://www.w3.org/ns/shacl#\",\n",
    "    \"omics\": \"http://data.boehringer.com/ontology/omics#\",\n",
    "    \"owl\": \"http://www.w3.org/2002/07/owl#\",    \n",
    "    \"cns\": \"http://customnamespace.com/n#\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "def create_backup(ttl_path):\n",
    "    # Read the input file\n",
    "    with open(ttl_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Define backup file path\n",
    "    backup_file_path = ttl_path + \".bak\"\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(ttl_path), exist_ok=True)\n",
    "\n",
    "    # Store backup file if it doesn't exist\n",
    "    if not os.path.exists(backup_file_path):\n",
    "        with open(backup_file_path, 'w') as file:\n",
    "            file.write(data)\n",
    "\n",
    "def handle_already_ns_already_exists(duplicate_ns, prefixes, ns_dict, g, changes):\n",
    "    for prefix, uri in prefixes.items():\n",
    "        # Normalize URI by ensuring it ends consistently\n",
    "        normalized_uri = uri.rstrip('#/')\n",
    "        for existing_prefix, existing_uri in ns_dict.items():\n",
    "            # Check for existing URI prefix with different naming\n",
    "            if existing_uri.rstrip('#/') == normalized_uri:\n",
    "                # Rename all usages of the existing prefix in the graph\n",
    "                namespace = rdflib.Namespace(uri)\n",
    "                for s, p, o in g.triples((None, None, None)):\n",
    "                    new_s = rename_if_matches(s, existing_uri, namespace)\n",
    "                    new_p = rename_if_matches(p, existing_uri, namespace)\n",
    "                    new_o = rename_if_matches(o, existing_uri, namespace)\n",
    "                    if (s, p, o) != (new_s, new_p, new_o):\n",
    "                        changes.append(((s, p, o), (new_s, new_p, new_o)))\n",
    "                # Update the namespace manager to use the new prefix\n",
    "                g.namespace_manager.bind(prefix, namespace, replace=True, override=True)\n",
    "\n",
    "def rename_if_matches(node, existing_uri, namespace):\n",
    "    if isinstance(node, rdflib.URIRef) and node.startswith(existing_uri):\n",
    "        new_local_name = str(node).replace(existing_uri, '').lstrip('#/')\n",
    "        return namespace[new_local_name]\n",
    "    return node\n",
    "\n",
    "def rebind_prefixes(g_path, prefixes):\n",
    "    create_backup(g_path)\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(g_path, format=\"turtle\")\n",
    "\n",
    "    ns_dict = {}\n",
    "    for x in g.namespace_manager.namespaces():\n",
    "        ns_dict[x[0]] = str(x[1])\n",
    "\n",
    "    changes = []\n",
    "\n",
    "    # Check if prefix or URI already exists in graph namespaces\n",
    "    for prefix, uri in prefixes.items():\n",
    "        if prefix in ns_dict and ns_dict[prefix] == uri:\n",
    "            print(f\"Prefix {prefix} with URI {uri} already correctly bound in the graph.\")\n",
    "            continue\n",
    "        elif any(uri.rstrip('#/') == ns.rstrip('#/') for ns in ns_dict.values()):\n",
    "            print(f\"URI {uri} already exists in the graph under a different prefix. Adjusting...\")\n",
    "            handle_already_ns_already_exists(prefix, {prefix: uri}, ns_dict, g, changes)\n",
    "        else:\n",
    "            g.bind(prefix, rdflib.Namespace(uri))\n",
    "            namespace = rdflib.Namespace(uri)\n",
    "\n",
    "            # Prepare to change all URIs in the graph that start with the given URI\n",
    "            for s, p, o in g.triples((None, None, None)):\n",
    "                new_s = s if not isinstance(s, rdflib.URIRef) or not s.startswith(uri[:-1]) else namespace[str(s).split('/')[-1]]\n",
    "                new_p = p if not isinstance(p, rdflib.URIRef) or not p.startswith(uri[:-1]) else namespace[str(p).split('/')[-1]]\n",
    "                new_o = o if not isinstance(o, rdflib.URIRef) or not o.startswith(uri[:-1]) else namespace[str(o).split('/')[-1]]\n",
    "\n",
    "                if (s, p, o) != (new_s, new_p, new_o):\n",
    "                    changes.append(((s, p, o), (new_s, new_p, new_o)))\n",
    "\n",
    "    # Apply changes\n",
    "    for old_triple, new_triple in changes:\n",
    "        g.remove(old_triple)\n",
    "        g.add(new_triple)\n",
    "\n",
    "    # Serialize and save the graph to a new Turtle file\n",
    "    g.serialize(destination=g_path, format=\"turtle\")\n",
    "\n",
    "\n",
    "\n",
    "rebind_prefixes(STUDY_TTL_PATH, prefixes)\n",
    "rebind_prefixes(VIOLATION_REPORT_TTL_PATH, prefixes)\n",
    "rebind_prefixes(OMICS_MODEL_TTL_PATH, prefixes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Enumerating Names to Unnamed Nodes\n",
    "example:\n",
    "\n",
    "[] a sh:ValidationResult -> <http://customnamespace.com/validationResult/1b29fdad-9eca-463f-b081-4e735785c238> a sh:ValidationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, BNode, Namespace\n",
    "import uuid\n",
    "import os\n",
    "import re\n",
    "\n",
    "def replace_with_incremented_name(match):\n",
    "    class_name = match.group(1)\n",
    "    return f\"cns:{class_name}1\"\n",
    "\n",
    "def process_ttl_file(ttl_path):\n",
    "    # Read the input file\n",
    "    with open(ttl_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Prepend '@prefix cns: <http://customnamespace.com/> .'\n",
    "    data = \"@prefix cns: <http://customnamespace.com/> .\\n\" + data\n",
    "    \n",
    "    # Apply textual replacements\n",
    "    data = re.sub(r\"\\[\\]\\s+a\\s+sh:(\\w+)\", replace_with_incremented_name, data)\n",
    "    counter = 1\n",
    "    while \"[]\" in data:\n",
    "        data = data.replace(\"[]\", f\"cns:NodeShape{counter}\", 1)\n",
    "        counter += 1\n",
    "\n",
    "    # Save the intermediate result to a temporary file\n",
    "    intermediate_path = ttl_path + \".tmp\"\n",
    "    with open(intermediate_path, 'w') as file:\n",
    "        file.write(data)\n",
    "\n",
    "    # Load the intermediate Turtle file into RDFLib\n",
    "    g = Graph()\n",
    "    g.parse(ttl_path, format=\"ttl\")\n",
    "\n",
    "    # Define namespaces\n",
    "    SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "    g.bind(\"sh\", SH)\n",
    "\n",
    "    for s,p,o in g:\n",
    "        print(s,p,o)\n",
    "\n",
    "    # Generate and replace blank nodes\n",
    "    unnamed_nodes = [node for node in g.all_nodes() if isinstance(node, BNode)]\n",
    "    for bnode in unnamed_nodes:\n",
    "        # Generate a new URI for the unnamed node using UUID\n",
    "        new_uri = URIRef(f\"http://customnamespace.com/validationResult/{uuid.uuid4()}\")\n",
    "        # Replace all occurrences of the blank node with the new URI in the graph\n",
    "        for p, o in g.predicate_objects(subject=bnode):\n",
    "            g.add((new_uri, p, o))\n",
    "            g.remove((bnode, p, o))\n",
    "        for s, p in g.subject_predicates(object=bnode):\n",
    "            g.add((s, p, new_uri))\n",
    "            g.remove((s, p, bnode))\n",
    "\n",
    "    # Serialize the modified graph back to Turtle and overwrite the original file\n",
    "    g.serialize(ttl_path, format=\"turtle\")\n",
    "\n",
    "\n",
    "    # Clean up: remove the intermediate file\n",
    "    os.remove(intermediate_path)\n",
    "\n",
    "# Process both files\n",
    "process_ttl_file(OMICS_MODEL_TTL_PATH)\n",
    "process_ttl_file(VIOLATION_REPORT_TTL_PATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Parse Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading study graph...\")\n",
    "study_g = Graph()\n",
    "study_g.parse(STUDY_TTL_PATH, format=\"ttl\")\n",
    "\n",
    "print(\"Reading violations graph...\")\n",
    "violations_g = Graph()\n",
    "violations_g.parse(VIOLATION_REPORT_TTL_PATH, format=\"ttl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">Careful: If the omics_model.ttl contains [] anywhere else, e.g., in a label, it will also be replaced there!</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiate NaN Cases - Predicate cannot Exist on Class vs. Predicate is Missing vs. Predicate Exists Even Though It Cannot\n",
    "A node can have an edge if\n",
    "\n",
    "s1 a sh:NodeShape\n",
    "\n",
    "s1 sh:targetClass node\n",
    "\n",
    "s1 sh:property s2\n",
    "\n",
    "s2 a sh:PropertyShape\n",
    "\n",
    "s2 sh:Path edge\n",
    "\n",
    "OR \n",
    "\n",
    "(\n",
    "node rdfs:subClassOf s3\n",
    "AND\n",
    "s3 can have the edge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from rdflib.namespace import Namespace\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read ontology into a graph\n",
    "ontology_g = Graph()\n",
    "ontology_g.parse(OMICS_MODEL_TTL_PATH, format=\"ttl\")\n",
    "\n",
    "# take namespace prefix for sh and omics from the graph\n",
    "sh = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "omics = Namespace(\"http://data.boehringer.com/ontology/omics#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "# define prefixes and corresponding namespaces\n",
    "prefixes = {\n",
    "    \"sh\": \"http://www.w3.org/ns/shacl#\",\n",
    "    \"omics\": \"http://data.boehringer.com/ontology/omics#\",\n",
    "    \"owl\": \"http://www.w3.org/2002/07/owl#\"\n",
    "}\n",
    "\n",
    "def convert_to_prefixed(graph, uri):\n",
    "    \"\"\"Convert a URI to its prefixed form using a rdflib graph's namespace manager.\"\"\"\n",
    "    prefix, namespace, name = graph.namespace_manager.compute_qname(uri)\n",
    "    return f\"{prefix}:{name}\"\n",
    "\n",
    "def select_direct_possible_edges(node):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?edge\n",
    "    WHERE {\n",
    "        ?ns a sh:NodeShape .\n",
    "        ?ns sh:targetClass %s .\n",
    "        ?ns sh:property ?ps .\n",
    "        ?ps a sh:PropertyShape .\n",
    "        ?ps sh:path ?edge .\n",
    "    }\n",
    "    \"\"\" % node\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_direct_parents(node):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?parent\n",
    "    WHERE {\n",
    "        %s rdfs:subClassOf ?parent .\n",
    "    }\n",
    "    \"\"\" % node\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_direct_children(node):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?child\n",
    "    WHERE {\n",
    "        ?child rdfs:subClassOf %s .\n",
    "    }\n",
    "    \"\"\" % node\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_roots():\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?root\n",
    "    WHERE {\n",
    "        ?root rdfs:subClassOf owl:Thing .\n",
    "    }\n",
    "    \"\"\"\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_root_classes():\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?class\n",
    "    WHERE {\n",
    "        ?class a owl:Class .\n",
    "        FILTER NOT EXISTS { ?class rdfs:subClassOf ?parent . }\n",
    "    }\n",
    "    \"\"\"\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "\n",
    "# use the functions\n",
    "roots = select_root_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyTree:\n",
    "    def __init__(self):\n",
    "        self.tree = {}\n",
    "\n",
    "    def add_node(self, node, edges, children):\n",
    "        if node not in self.tree:\n",
    "            self.tree[node] = {'edges': edges, 'children': children}\n",
    "\n",
    "    def add_child(self, parent, child):\n",
    "        if parent in self.tree:\n",
    "            self.tree[parent]['children'].append(child)\n",
    "        else:\n",
    "            self.tree[parent] = {'edges': [], 'children': [child]}\n",
    "\n",
    "def aggregate_edges(node, tree):\n",
    "    edges = set(tree[node]['edges'])\n",
    "    queue = [parent_node for parent_node in tree if node in tree[parent_node]['children']]\n",
    "    while queue:\n",
    "        parent_node = queue.pop(0)\n",
    "        edges |= set(tree[parent_node]['edges'])\n",
    "        queue += [grand_parent for grand_parent in tree if parent_node in tree[grand_parent]['children']]\n",
    "    return edges\n",
    "\n",
    "def create_aggregated_edges_dict(tree):\n",
    "    agg_edges_dict = {}\n",
    "    for node in tree.keys():\n",
    "        agg_edges_dict[node] = aggregate_edges(node, tree)\n",
    "    return agg_edges_dict\n",
    "\n",
    "def convert_to_full_uri(graph, abbreviated_uri):\n",
    "    \"\"\"Converts a prefixed URI to a full URI using a rdflib graph's namespace manager.\"\"\"\n",
    "    prefix, uri = abbreviated_uri.split(':')\n",
    "    namespace = graph.namespace_manager.store.namespace(prefix)\n",
    "    return namespace + uri\n",
    "\n",
    "def remove_prefix(uri):\n",
    "    if ':' in uri:\n",
    "        return uri.split(':', 1)[1]\n",
    "    return uri\n",
    "\n",
    "def build_tree(root_nodes, ontology_tree):\n",
    "    queue = root_nodes\n",
    "    while queue:\n",
    "        current_node = queue.pop(0)\n",
    "        # unprefixed_node = remove_prefix(current_node)\n",
    "        print('unprefixed_node', current_node)\n",
    "        edges = select_direct_possible_edges(current_node)\n",
    "        children = select_direct_children(current_node)\n",
    "\n",
    "        ontology_tree.add_node(current_node, edges, children)\n",
    "\n",
    "        for child in children:\n",
    "            queue.append(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = select_root_classes()\n",
    "ontology_tree = OntologyTree()\n",
    "build_tree(roots, ontology_tree)\n",
    "agg_edges_dict = create_aggregated_edges_dict(ontology_tree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_all_focus_nodes_and_classes(graph):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?node ?class\n",
    "    WHERE {\n",
    "        ?node a ?class .\n",
    "    }\n",
    "    \"\"\"\n",
    "    results = graph.query(select_statement)\n",
    "    focus_nodes_classes_dict = {str(result[0]): str(result[1]) for result in results}\n",
    "    return focus_nodes_classes_dict\n",
    "\n",
    "\n",
    "focus_nodes_class_dict = select_all_focus_nodes_and_classes(study_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabularize & Join Study + Violations Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace\n",
    "from rdflib.namespace import NamespaceManager\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "empty_edge_string = \"EdgeNotPresent\"\n",
    "\n",
    "SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "\n",
    "def count_violations(violations: Graph):\n",
    "    d_focus_node_d_source_shape_counts = defaultdict(lambda: defaultdict(int))\n",
    "    d_violation_focus_node = defaultdict((str))\n",
    "    d_violation_source_shape = defaultdict((str))\n",
    "    violation_list = []\n",
    "\n",
    "    for s, p, o in tqdm(violations, desc=\"Processing Graph Once\"):\n",
    "        if p == SH.sourceShape:\n",
    "            d_violation_source_shape[str(s)] = str(o)\n",
    "            violation_list += [str(o)]\n",
    "        elif p == SH.focusNode:\n",
    "            d_violation_focus_node[str(s)] = str(o)\n",
    "\n",
    "    # for key in violation_focus_node dict\n",
    "    for key in tqdm(d_violation_focus_node, desc=\"Going Over Violation Instances\"):\n",
    "        # init with 1 or add 1 if already exists\n",
    "        d_focus_node_d_source_shape_counts[d_violation_focus_node[key]][d_violation_source_shape[key]] += 1\n",
    "\n",
    "    violation_list = list(set(violation_list))\n",
    "    return d_focus_node_d_source_shape_counts, violation_list\n",
    "\n",
    "\n",
    "def save_violation_list(violation_list):\n",
    "    with open(VIOLATION_LIST_FILE, 'w') as f:\n",
    "        json.dump(violation_list, f)  \n",
    "\n",
    "\n",
    "def create_study_dataframe(study: Graph, d_focus_node_d_source_shape_counts, focus_nodes_class_dict, agg_edges_dict):\n",
    "    # for convenience\n",
    "    pre = lambda x: convert_to_prefixed(ontology_g, str(x))\n",
    "\n",
    "    # create an empty study_df, fill the violation counts\n",
    "    study_df = pd.DataFrame(d_focus_node_d_source_shape_counts).T.fillna(0)\n",
    "\n",
    "    count_not_skipped, count_skipped, count_allowed, count_not_allowed_exists, count_not_allowed_empty = 0, 0, 0, 0, 0\n",
    "\n",
    "    # iterate all study triples\n",
    "    for s, p, o in tqdm(study, desc=\"Processing study\"):\n",
    "        # if s is not in d_focus_node_d_source_shape_counts, continue as it is not a focus node\n",
    "        if str(s) not in d_focus_node_d_source_shape_counts:\n",
    "            count_skipped += 1\n",
    "            continue\n",
    "        count_not_skipped += 1\n",
    "\n",
    "        # if p is not in study_df.columns, add it and set it to empty_edge_string for all rows because so far there has not been a value for it for anything, and future values will overwrite it anyway\n",
    "        if str(p) not in study_df.columns:\n",
    "            study_df[str(p)] = empty_edge_string\n",
    "\n",
    "        # if object is empty throw exception and print details\n",
    "        if str(o) == \"\":\n",
    "            print(\"empty object for \" + str(s) + \" \" + str(p))\n",
    "            raise Exception(\"empty object for \" + str(s) + \" \" + str(p))\n",
    "\n",
    "        # get class of focus node\n",
    "        focus_node_class = focus_nodes_class_dict.get(str(s), None)\n",
    "        assert focus_node_class is not None, f\"The focus_node_class for {str(s)} was None, but expected a value.\"\n",
    "        # get allowed edges\n",
    "        focus_node_edges = agg_edges_dict.get(convert_to_prefixed(ontology_g, focus_node_class), set())\n",
    "        # assert len(focus_node_edges) > 0\n",
    "\n",
    "        def write_object_to_study_df(study_df, s, p, o):\n",
    "            current_value = study_df.at[str(s), str(p)]\n",
    "            # Check if the current cell already contains a value that is not empty_edge_string\n",
    "            if current_value == empty_edge_string:\n",
    "                # If the cell is empty, directly assign the new value\n",
    "                study_df.at[str(s), str(p)] = str(o)\n",
    "            else:\n",
    "                # If the cell already has a value, ensure it's a list and append the new value\n",
    "                if not isinstance(current_value, list):\n",
    "                    current_value = [current_value] if current_value != str(o) else current_value\n",
    "                if isinstance(current_value, list):\n",
    "                    current_value.append(str(o))\n",
    "                # Update the DataFrame cell with the new value\n",
    "                study_df.at[str(s), str(p)] = current_value\n",
    "\n",
    "\n",
    "        # if edge is allowed (and object is present otherwise we wouldn't get here), set value study_df.at[str(s), str(p)] = str(o)\n",
    "        if pre(p) in focus_node_edges:\n",
    "            write_object_to_study_df(study_df, s, p, o)\n",
    "            count_allowed += 1\n",
    "        # if edge is not allowed, and there is no object, set value study_df.at[str(s), str(p)] = empty_edge_string\n",
    "        elif pre(p) not in focus_node_edges and str(o) == \"\":\n",
    "            study_df.at[str(s), str(p)] = empty_edge_string\n",
    "            count_not_allowed_empty += 1\n",
    "        # if edge is not allowed, and there is an object, set value study_df.at[str(s), str(p)] = \"InvalidEdgeWUnexpectedEdgeWithDataithData: str(o)\"\n",
    "        elif pre(p) not in focus_node_edges and str(o) != \"\":\n",
    "            # study_df.at[str(s), str(p)] = \"UnexpectedEdgeWithData: \" + str(o)\n",
    "            write_object_to_study_df(study_df, s, p, o)\n",
    "            count_not_allowed_exists += 1\n",
    "        else:\n",
    "            raise Exception(\"should not happen: \" + str(s) + \" \" + str(p) + \" \" + str(o))\n",
    "\n",
    "    print(f\"count_skipped = {count_skipped}\")\n",
    "    print(f\"count_not_skipped = {count_not_skipped}\")\n",
    "    print(f\"count_allowed = {count_allowed}\")\n",
    "    print(f\"count_not_allowed_exists = {count_not_allowed_exists}\")\n",
    "    print(f\"count_not_allowed_empty = {count_not_allowed_empty}\")\n",
    "\n",
    "    assert study_df.isin(['']).sum().sum() == 0\n",
    "    assert study_df.isnull().sum().sum() == 0, f\"study_df.isnull().sum().sum() = {study_df.isnull().sum().sum()}\"\n",
    "\n",
    "    # add focus_node column, set it to the study_df.index, and move it to the very left of the dataframe\n",
    "    study_df['focus_node'] = study_df.index\n",
    "    cols = study_df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]  # move the last column to the first\n",
    "    study_df = study_df[cols]\n",
    "\n",
    "    return study_df\n",
    "\n",
    "\n",
    "def save_study_dataframe(study_df):\n",
    "    study_df.to_csv(STUDY_CSV_FILE)\n",
    "\n",
    "def replace_empty_edge_with_empty_array(df, empty_edge_string=\"EdgeNotPresent\"):\n",
    "    # Iterate through each column of the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Determine if the column is a set column (contains at least one list)\n",
    "        is_set_column = any(df[column].apply(lambda x: isinstance(x, list)))\n",
    "        print(is_set_column, column)\n",
    "        \n",
    "        # Apply conditional replacement based on whether the column is a set column\n",
    "        if is_set_column:\n",
    "            # For set columns, replace empty_edge_string with an empty list\n",
    "            df[column] = df[column].apply(lambda x: [] if x == empty_edge_string else x)\n",
    "        else:\n",
    "            # For non-set columns, ensure that empty_edge_string remains for missing values\n",
    "            # This else block might not perform any action, but it's included for clarity.\n",
    "            # Adjust or remove based on your specific requirements or if further logic is needed.\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def post_process_convert_to_arrays(df, empty_edge_string=\"EdgeNotPresent\"):\n",
    "    for column in df.columns:\n",
    "        # Check if any cell in the column contains a list\n",
    "        contains_list = df[column].apply(lambda x: isinstance(x, list)).any()\n",
    "        if contains_list:\n",
    "            # Convert all non-list cells in the column into a list\n",
    "            df[column] = df[column].apply(lambda x: [x] if not isinstance(x, list) else x)\n",
    "        else:\n",
    "            # For columns without lists, no action is needed\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "# Function to wrap single elements into arrays\n",
    "def wrap_in_array(element):\n",
    "    if not isinstance(element, list):\n",
    "        return [element]\n",
    "    return element\n",
    "\n",
    "\n",
    "def tabularize_graphs(study: Graph, violations: Graph):\n",
    "    print('study graph triples:', len(study))  # type: ignore\n",
    "    print('violations graph triples:', len(violations))  # type: ignore\n",
    "    d_focus_node_d_source_shape_counts, violation_list = count_violations(violations)\n",
    "    save_violation_list(violation_list)\n",
    "    study_df = create_study_dataframe(study, d_focus_node_d_source_shape_counts, focus_nodes_class_dict, agg_edges_dict)\n",
    "    study_df = post_process_convert_to_arrays(study_df, empty_edge_string)\n",
    "    rdf_type_string = 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'\n",
    "    # always turn rdf:type column into a set column filled with arrays\n",
    "    study_df[rdf_type_string] = study_df[rdf_type_string].apply(wrap_in_array)\n",
    "\n",
    "    # study_df = replace_empty_edge_with_empty_array(study_df, empty_edge_string)\n",
    "    save_study_dataframe(study_df)\n",
    "    return d_focus_node_d_source_shape_counts\n",
    "\n",
    "\n",
    "d_focus_node_d_source_shape_counts = tabularize_graphs(study_g, violations_g)\n",
    "\n",
    "study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "study_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests for Violation Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from rdflib import Graph, Literal, Namespace, BNode, URIRef\n",
    "\n",
    "SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "\n",
    "class TestCountViolations(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.violations = Graph()\n",
    "\n",
    "    def test_count_violations_node_with_multiple_and_node_with_double(self):\n",
    "        self.focusNode1 = BNode()\n",
    "        self.focusNode2 = BNode()\n",
    "        self.sourceShape1 = Literal('shape1')\n",
    "        self.sourceShape2 = Literal('shape2')\n",
    "        self.violation1 = BNode()\n",
    "        self.violation2 = BNode()\n",
    "        self.violation3 = BNode()\n",
    "        self.violation4 = BNode()\n",
    "\n",
    "        # Add triples to the violations graph\n",
    "        self.violations.add((self.violation1, SH.focusNode, self.focusNode1))\n",
    "        self.violations.add((self.violation1, SH.sourceShape, self.sourceShape1))\n",
    "\n",
    "        self.violations.add((self.violation2, SH.focusNode, self.focusNode2))\n",
    "        self.violations.add((self.violation2, SH.sourceShape, self.sourceShape2))\n",
    "\n",
    "        self.violations.add((self.violation3, SH.focusNode, self.focusNode1))\n",
    "        self.violations.add((self.violation3, SH.sourceShape, self.sourceShape1))\n",
    "\n",
    "        self.violations.add((self.violation4, SH.focusNode, self.focusNode2))\n",
    "        self.violations.add((self.violation4, SH.sourceShape, self.sourceShape1))\n",
    "        violation_counts, violation_list = count_violations(self.violations)\n",
    "        expected_counts = {\n",
    "            str(self.focusNode1): {str(self.sourceShape1): 2},\n",
    "            str(self.focusNode2): {str(self.sourceShape2): 1, str(self.sourceShape1): 1}\n",
    "        }\n",
    "        expected_list = [str(self.sourceShape1), str(self.sourceShape2)] \n",
    "        \n",
    "        self.assertEqual(violation_counts, expected_counts)\n",
    "        self.assertEqual(set(violation_list), set(expected_list))  # Use set to ignore order\n",
    "\n",
    "    def test_count_violations_basic(self):\n",
    "        self.focusNode1 = BNode()\n",
    "        self.focusNode2 = BNode()\n",
    "        self.focusNode3 = BNode()\n",
    "        self.sourceShape1 = Literal('shape1')\n",
    "        self.sourceShape2 = Literal('shape2')\n",
    "        self.sourceShape3 = Literal('shape3')\n",
    "        self.violation1 = BNode()\n",
    "        self.violation2 = BNode()\n",
    "        self.violation3 = BNode()\n",
    "\n",
    "        # Add triples to the violations graph\n",
    "        self.violations.add((self.violation1, SH.focusNode, self.focusNode1))\n",
    "        self.violations.add((self.violation1, SH.sourceShape, self.sourceShape1))\n",
    "\n",
    "        self.violations.add((self.violation2, SH.focusNode, self.focusNode2))\n",
    "        self.violations.add((self.violation2, SH.sourceShape, self.sourceShape2))\n",
    "\n",
    "        self.violations.add((self.violation3, SH.focusNode, self.focusNode3))\n",
    "        self.violations.add((self.violation3, SH.sourceShape, self.sourceShape3))\n",
    "\n",
    "        violation_counts, violation_list = count_violations(self.violations)\n",
    "        expected_counts = {\n",
    "            str(self.focusNode1): {str(self.sourceShape1): 1},\n",
    "            str(self.focusNode2): {str(self.sourceShape2): 1},\n",
    "            str(self.focusNode3): {str(self.sourceShape3): 1}\n",
    "        }\n",
    "        expected_list = [str(self.sourceShape1), str(self.sourceShape2), str(self.sourceShape3)] \n",
    "        \n",
    "        self.assertEqual(violation_counts, expected_counts)\n",
    "        self.assertEqual(set(violation_list), set(expected_list))  # Use set to ignore order\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestCountViolations)\n",
    "unittest.TextTestRunner().run(suite)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbreviation using QNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import split_uri\n",
    "\n",
    "def namespace_in_nsm(nsm, namespace):\n",
    "    \"\"\"\n",
    "    Checks if a given namespace is in the provided NamespaceManager.\n",
    "\n",
    "    Parameters:\n",
    "    nsm (NamespaceManager): the NamespaceManager object to check\n",
    "    namespace (str): the namespace to look for in the NamespaceManager\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the namespace is in the NamespaceManager, False otherwise\n",
    "    \"\"\"\n",
    "    for prefix, ns in nsm.namespaces():\n",
    "        if str(ns) == str(namespace):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_qname(nsm, uri):\n",
    "    \"\"\"\n",
    "    Returns a QName for a given URI if its namespace is in the provided NamespaceManager.\n",
    "    If the namespace isn't found in the NamespaceManager, it returns the original URI.\n",
    "\n",
    "    Parameters:\n",
    "    nsm (NamespaceManager): the NamespaceManager object to use\n",
    "    uri (str): the URI to transform into a QName\n",
    "\n",
    "    Returns:\n",
    "    str: a QName if the namespace of the URI is in the NamespaceManager, the original URI otherwise\n",
    "    \"\"\"\n",
    "    namespace, name = split_uri(uri)\n",
    "    if namespace_in_nsm(nsm, namespace):\n",
    "        try:\n",
    "            prefix, _, _ = nsm.compute_qname(uri)\n",
    "            return f\"{prefix}:{name}\"\n",
    "        except Exception as e:\n",
    "            return uri\n",
    "    else:\n",
    "        return uri\n",
    "    \n",
    "def abbreviate_cell_value(nsm, cell_value):\n",
    "    \"\"\"\n",
    "    Attempts to abbreviate URIs in a cell that may contain a string representation\n",
    "    of a list of URIs or a single URI. If abbreviation is not possible, the original\n",
    "    cell value is returned.\n",
    "    \n",
    "    Parameters:\n",
    "    nsm (NamespaceManager): The NamespaceManager to use for abbreviation.\n",
    "    cell_value (str): The cell value to abbreviate.\n",
    "    \n",
    "    Returns:\n",
    "    str: The abbreviated cell value if possible; otherwise, the original cell value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to evaluate the cell_value as a list or single URI\n",
    "        uris = ast.literal_eval(cell_value)\n",
    "        # Handle list of URIs\n",
    "        if isinstance(uris, list):\n",
    "            abbreviated_uris = [get_qname(nsm, uri) for uri in uris]\n",
    "            return str(abbreviated_uris)\n",
    "        else:\n",
    "            # Handle a single URI\n",
    "            return get_qname(nsm, uris)\n",
    "    except:\n",
    "        # If the above fails (either due to eval or not being a URI), return original\n",
    "        try:\n",
    "            # Attempt to directly abbreviate assuming it's a single URI\n",
    "            return get_qname(nsm, cell_value)\n",
    "        except:\n",
    "            # If all else fails, return the cell value as is\n",
    "            return cell_value\n",
    "\n",
    "\n",
    "def abbreviate_using_namespaces(study_graph: Graph, violations_graph: Graph):\n",
    "    \"\"\"\n",
    "    Replaces URIs in a DataFrame and a list of violations with QNames if their namespaces\n",
    "    are found in two provided RDF graphs. Saves the updated DataFrame and the list back to disk.\n",
    "\n",
    "    Parameters:\n",
    "    study_graph (Graph): the first RDF graph to use for namespaces\n",
    "    violations_graph (Graph): the second RDF graph to use for namespaces\n",
    "    \"\"\"\n",
    "\n",
    "    # load violation_list\n",
    "    with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "        violation_list = json.load(f)\n",
    "\n",
    "    # read study_df tabularized data\n",
    "    study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "\n",
    "    # Create a new graph to combine namespaces\n",
    "    combined_graph = Graph()\n",
    "\n",
    "    # Get namespaces from both graphs and bind them to the combined graph\n",
    "    for prefix, ns_uri in study_graph.namespace_manager.namespaces():\n",
    "        combined_graph.namespace_manager.bind(prefix, ns_uri)\n",
    "\n",
    "    for prefix, ns_uri in violations_graph.namespace_manager.namespaces():\n",
    "        combined_graph.namespace_manager.bind(prefix, ns_uri)\n",
    "\n",
    "    # Now use the NamespaceManager of the combined graph\n",
    "    nsm_combined = NamespaceManager(combined_graph)\n",
    "\n",
    "    # Change column names\n",
    "    for col in study_df.columns:\n",
    "        try:\n",
    "            study_df.rename(columns={col: get_qname(nsm_combined, col)}, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "     # Change cell values to handle arrays in string form\n",
    "    for col in study_df.columns:\n",
    "        for idx in study_df.index:\n",
    "            cell_value = study_df.loc[idx, col]\n",
    "            # Check if the cell contains a string that needs abbreviation\n",
    "            if isinstance(cell_value, str):\n",
    "                study_df.loc[idx, col] = abbreviate_cell_value(nsm_combined, cell_value)\n",
    "\n",
    "    # Change indices and cell values\n",
    "    for col in study_df.columns:\n",
    "        for idx in study_df.index:\n",
    "            try:\n",
    "                study_df.loc[idx, col] = get_qname(nsm_combined, study_df.loc[idx, col])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # change index\n",
    "    for idx in study_df.index:\n",
    "        try:\n",
    "            study_df.rename(index={idx: get_qname(nsm_combined, str(idx))}, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # change violation_list\n",
    "    for i in range(len(violation_list)):\n",
    "        try:\n",
    "            violation_list[i] = get_qname(nsm_combined, violation_list[i])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # store tabular graph as csv\n",
    "    study_df.to_csv(STUDY_CSV_FILE)    \n",
    "\n",
    "    # store violation_list as json\n",
    "    with open(VIOLATION_LIST_FILE, 'w') as f:\n",
    "        json.dump(violation_list, f)\n",
    "\n",
    "abbreviate_using_namespaces(study_g, violations_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "    violation_list = json.load(f)\n",
    "\n",
    "print('unique violations ('+str(len(violation_list))+'):\\n')\n",
    "print('\\n'.join([str(v) for v in violation_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "study_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Violation Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def create_embedding():\n",
    "    # read study.csv\n",
    "    study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "    \n",
    "    # read violation_list.json\n",
    "    with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "        violation_columns = json.load(f)\n",
    "    \n",
    "    view_df = study_df[violation_columns]  # view of study dataframe with only violation columns\n",
    "\n",
    "    if len(view_df) > 1:\n",
    "        print(\"Creating UMAP embedding... (this may take some time)\")\n",
    "        reducer = umap.UMAP(n_neighbors=int(np.sqrt(len(view_df))), min_dist=0.1, n_components=2, random_state=0, verbose=True)\n",
    "        result = reducer.fit_transform(view_df)  # 2D projection\n",
    "    else:\n",
    "        print(\"Dataframe has only 1 row, skipping UMAP embedding.\")\n",
    "        result = np.zeros((1, 2))  # Return a default embedding for the single data point\n",
    "\n",
    "    # Check if the result is a tuple and unpack accordingly\n",
    "    if isinstance(result, tuple):\n",
    "        embedding, *_ = result\n",
    "    else:\n",
    "        embedding = result\n",
    "\n",
    "    # Convert to dense numpy array if it's a sparse matrix\n",
    "    if isinstance(embedding, coo_matrix):\n",
    "        embedding = embedding.toarray()\n",
    "\n",
    "    # Assuming that the embedding is now a NumPy array, based on the docstring\n",
    "    study_df[\"x\"] = embedding[:, 0]\n",
    "    study_df[\"y\"] = embedding[:, 1]\n",
    "\n",
    "    study_df.to_csv(STUDY_CSV_FILE)\n",
    "    print(\"UMAP embedding created and saved to\", STUDY_CSV_FILE)\n",
    "\n",
    "\n",
    "def plot_embedding():\n",
    "    study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "\n",
    "    plt.scatter(study_df[\"x\"], study_df[\"y\"])\n",
    "    plt.show()\n",
    "\n",
    "create_embedding()\n",
    "plot_embedding()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace URIs with Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from rdflib import URIRef\n",
    "\n",
    "# list of label predicates in ascending order of priority\n",
    "label_predicates = [\n",
    "    # URIRef('http://data.boehringer.com/ontology/omics/hasValue'),\n",
    "    # URIRef('http://data.boehringer.com/ontology/omics/name'),\n",
    "    URIRef('http://www.w3.org/2004/02/skos/core#prefLabel')\n",
    "]\n",
    "\n",
    "# read tabularized graph .csv as a dataframe, and violations list .json as a list\n",
    "study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "\n",
    "with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "    violation_columns = json.load(f)\n",
    "\n",
    "# Create a new graph to combine namespaces\n",
    "combined_graph = Graph()\n",
    "\n",
    "# Get namespaces from both graphs and bind them to the combined graph\n",
    "for uri, ns_uri in study_g.namespace_manager.namespaces():\n",
    "    combined_graph.namespace_manager.bind(uri, ns_uri)\n",
    "\n",
    "for uri, ns_uri in violations_g.namespace_manager.namespaces():\n",
    "    combined_graph.namespace_manager.bind(uri, ns_uri)\n",
    "\n",
    "# Now use the NamespaceManager of the combined graph\n",
    "nsm_combined = NamespaceManager(combined_graph)\n",
    "\n",
    "# create a dictionary of {s: o} pairs for translating source shapes to their labels\n",
    "label_dict = {}\n",
    "for label_predicate in label_predicates:\n",
    "    temp_dict = {str(s): str(o) for s, p, o in study_g.triples((None, label_predicate, None))}\n",
    "    # replace the temp_dict keys with their corresponding QNames\n",
    "    temp_dict = {get_qname(nsm_combined, k): v for k, v in temp_dict.items()}\n",
    "    # Update label_dict with temp_dict, overwriting existing keys\n",
    "    label_dict.update(temp_dict)\n",
    "\n",
    "# replace all column names, indices, and cell values in the dataframe with their corresponding labels\n",
    "study_df.columns = [label_dict.get(col, col) for col in study_df.columns]\n",
    "study_df.index = pd.Index([label_dict.get(idx, idx) for idx in study_df.index])\n",
    "for col in study_df.columns:\n",
    "    study_df[col] = study_df[col].apply(lambda x: label_dict.get(x, x))\n",
    "\n",
    "# replace all dictionary keys and values with their corresponding labels\n",
    "violation_columns = [label_dict.get(x, x) for x in violation_columns]\n",
    "\n",
    "# store dataframe back to .csv and violations list back to .json\n",
    "study_df.to_csv(STUDY_CSV_FILE)\n",
    "with open(VIOLATION_LIST_FILE, 'w') as f:\n",
    "    json.dump(violation_columns, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Union of Ontology and Violation Report Exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef, RDFS\n",
    "\n",
    "def replace_uris_with_labels(data, label_dict, data_type=\"graph\"):\n",
    "    if data_type == \"graph\":\n",
    "        # Iterate through each triple\n",
    "        for s, p, o in list(data.triples((None, None, None))):\n",
    "            new_s = URIRef(label_dict.get(str(s), s))\n",
    "            new_o = URIRef(label_dict.get(str(o), o))\n",
    "            \n",
    "            if new_s != s or new_o != o:\n",
    "                data.remove((s, p, o))\n",
    "                data.add((new_s, p, new_o))\n",
    "                \n",
    "    elif data_type == \"dict\":\n",
    "        new_data = {}\n",
    "        for key, values in data.items():\n",
    "            new_key = label_dict.get(key, key)\n",
    "            \n",
    "            # If the values are in set form\n",
    "            if isinstance(values, set):\n",
    "                new_values = {label_dict.get(val, val) for val in values}\n",
    "                \n",
    "            # If the values are in dictionary form with concatenated string keys\n",
    "            elif isinstance(values, dict):\n",
    "                new_values = {}\n",
    "                for po_str, count in values.items():\n",
    "                    if '__' in po_str:\n",
    "                        p, o = po_str.split('__')\n",
    "                        new_p = label_dict.get(p, p)\n",
    "                        new_o = label_dict.get(o, o)\n",
    "                        new_po_str = f\"{new_p}__{new_o}\"\n",
    "                        new_values[new_po_str] = count\n",
    "                    else:\n",
    "                        new_po_str = label_dict.get(po_str, po_str)\n",
    "                        new_values[new_po_str] = count\n",
    "            \n",
    "            new_data[new_key] = new_values\n",
    "            \n",
    "        return new_data\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def add_labels_to_graph(graph, label_dict):\n",
    "    for uri, label in label_dict.items():\n",
    "        graph.add((URIRef(uri), URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'), Literal(label)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "import json\n",
    "from routers.utils import get_violation_report_exemplars\n",
    "\n",
    "ontology_union_violation_exemplars_g, edge_count_dict, focus_node_exemplar_dict, exemplar_focus_node_dict, violation_exemplar_dict = get_violation_report_exemplars(ontology_g, violations_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for k,v in violation_exemplar_dict.items():\n",
    "    if c == 10:\n",
    "        break\n",
    "    print(k, v)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also replace in joined .ttl of ontology and violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the graph\n",
    "ontology_union_violation_exemplars_g = add_labels_to_graph(ontology_union_violation_exemplars_g, label_dict)\n",
    "\n",
    "# For the dictionaries\n",
    "edge_count_dict = replace_uris_with_labels(edge_count_dict, label_dict, data_type=\"dict\")\n",
    "focus_node_exemplar_dict = replace_uris_with_labels(focus_node_exemplar_dict, label_dict, data_type=\"dict\")\n",
    "exemplar_focus_node_dict = replace_uris_with_labels(exemplar_focus_node_dict, label_dict, data_type=\"dict\")\n",
    "# if necessary add code for replacing uris with labels in violation_exemplar_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_union_violation_exemplars_g.serialize(destination=OMICS_MODEL_UNION_VIOLATION_EXEMPLAR_TTL_PATH, format=\"ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from routers.utils import save_nested_counts_dict_json, save_lists_dict\n",
    "\n",
    "save_nested_counts_dict_json(edge_count_dict, EXEMPLAR_EDGE_COUNT_DICT_PATH)\n",
    "save_lists_dict(focus_node_exemplar_dict, FOCUS_NODE_EXEMPLAR_DICT_PATH)\n",
    "save_lists_dict(exemplar_focus_node_dict, EXEMPLAR_FOCUS_NODE_DICT_PATH)\n",
    "save_nested_counts_dict_json(violation_exemplar_dict, VIOLATION_EXEMPLAR_DICT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
