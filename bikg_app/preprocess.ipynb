{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rdflib import Graph, Namespace\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "VIOLATION_LIST_FILE = \"./json/violation_list.json\" # path to store the violation list\n",
    "STUDY_CSV_FILE = \"./csv/study.csv\" # path to store the tabluraized study csv file\n",
    "STUDY_TTL_PATH = './ttl/study.ttl' # contains the actual study data\n",
    "OMICS_MODEL_TTL_PATH = './ttl/omics_model.ttl' # contains the ontology\n",
    "OMICS_MODEL_UNION_VIOLATION_EXEMPLAR_TTL_PATH = './ttl/omics_model_union_violation_exemplar.ttl' # contains the union of the ontology and the violation exemplars\n",
    "VIOLATION_REPORT_TTL_PATH = './ttl/violation_report.ttl' # contains the shacl violation test case result\n",
    "EXEMPLAR_EDGE_COUNT_DICT_PATH = './json/exemplar_edge_count_dict.json' # contains the nested dict of exemplar edge counts\n",
    "FOCUS_NODE_EXEMPLAR_DICT_PATH = './json/focus_node_exemplar_dict.json' # contains the dict of exemplars per focus node\n",
    "EXEMPLAR_FOCUS_NODE_DICT_PATH = './json/exemplar_focus_node_dict.json' # contains the dict of focus nodes per exemplar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Parse Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading study graph...\")\n",
    "study_g = Graph()\n",
    "study_g.parse(STUDY_TTL_PATH, format=\"ttl\")\n",
    "\n",
    "print(\"Reading violations graph...\")\n",
    "violations_g = Graph()\n",
    "violations_g.parse(VIOLATION_REPORT_TTL_PATH, format=\"ttl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Enumarting Names to Unnamed NodeShapes\n",
    "example: [] a sh:NodeShape -> NodeShape1 a sh:NodeShape\n",
    "etc\n",
    "this could be done with rdflib but then when storing the graph to a .ttl file it would automatically mess with the namespaces and remove unused ones\n",
    "so until these names are provided by BI, we decided to do simple string replacement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">Careful: If the omics_model.ttl contains [] anywhere else, e.g., in a label, it will also be replaced there!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read the input file\n",
    "with open(OMICS_MODEL_TTL_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Define backup file path\n",
    "backup_file_path = OMICS_MODEL_TTL_PATH + \".bak\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(OMICS_MODEL_TTL_PATH), exist_ok=True)\n",
    "\n",
    "# store backup file\n",
    "with open(backup_file_path, 'w') as file:\n",
    "    file.write(data)\n",
    "\n",
    "# Initialize the counter\n",
    "counter = 1\n",
    "\n",
    "# Prepend '@prefix cns: <http://custom_node_shapes/> .'\n",
    "data = \"@prefix cns: <http://customnamespace.com/> .\\n\" + data\n",
    "\n",
    "# Replace occurrences of [] with NodeShape{counter}\n",
    "while \"[]\" in data:\n",
    "    data = data.replace(\"[]\", f\"cns:NodeShape{counter}\", 1)\n",
    "    counter += 1\n",
    "\n",
    "# Write the updated string to the new file\n",
    "with open(OMICS_MODEL_TTL_PATH, 'w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiate NaN Cases - Predicate cannot Exist on Class vs. Predicate is Missing vs. Predicta Exists Even Though It Cannot\n",
    "A node can have an edge if\n",
    "\n",
    "s1 a sh:NodeShape\n",
    "\n",
    "s1 sh:targetClass node\n",
    "\n",
    "s1 sh:property s2\n",
    "\n",
    "s2 a sh:PropertyShape\n",
    "\n",
    "s2 sh:Path edge\n",
    "\n",
    "OR \n",
    "\n",
    "(\n",
    "node rdfs:subClassOf s3\n",
    "AND\n",
    "s3 can have the edge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from rdflib.namespace import Namespace\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read ontology into a graph\n",
    "ontology_g = Graph()\n",
    "ontology_g.parse(OMICS_MODEL_TTL_PATH, format=\"ttl\")\n",
    "\n",
    "# take namespace prefix for sh and omics from the graph\n",
    "sh = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "omics = Namespace(\"http://data.boehringer.com/ontology/omics#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "# define prefixes and corresponding namespaces\n",
    "prefixes = {\n",
    "    \"sh\": \"http://www.w3.org/ns/shacl#\",\n",
    "    \"omics\": \"http://data.boehringer.com/ontology/omics#\",\n",
    "    \"owl\": \"http://www.w3.org/2002/07/owl#\"\n",
    "}\n",
    "\n",
    "def convert_to_prefixed(graph, uri):\n",
    "    \"\"\"Convert a URI to its prefixed form using a rdflib graph's namespace manager.\"\"\"\n",
    "    prefix, namespace, name = graph.namespace_manager.compute_qname(uri)\n",
    "    return f\"{prefix}:{name}\"\n",
    "\n",
    "def select_direct_possible_edges(node):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?edge\n",
    "    WHERE {\n",
    "        ?ns a sh:NodeShape .\n",
    "        ?ns sh:targetClass omics:%s .\n",
    "        ?ns sh:property ?ps .\n",
    "        ?ps a sh:PropertyShape .\n",
    "        ?ps sh:path ?edge .\n",
    "    }\n",
    "    \"\"\" % node\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_direct_parents(node):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?parent\n",
    "    WHERE {\n",
    "        omics:%s rdfs:subClassOf ?parent .\n",
    "    }\n",
    "    \"\"\" % node\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_direct_children(node):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?child\n",
    "    WHERE {\n",
    "        ?child rdfs:subClassOf omics:%s .\n",
    "    }\n",
    "    \"\"\" % node\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_roots():\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?root\n",
    "    WHERE {\n",
    "        ?root rdfs:subClassOf owl:Thing .\n",
    "    }\n",
    "    \"\"\"\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "def select_root_classes():\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?class\n",
    "    WHERE {\n",
    "        ?class a owl:Class .\n",
    "        FILTER NOT EXISTS { ?class rdfs:subClassOf ?parent . }\n",
    "    }\n",
    "    \"\"\"\n",
    "    return [str(result[0].n3(ontology_g.namespace_manager)) for result in ontology_g.query(select_statement)] # type: ignore\n",
    "\n",
    "\n",
    "# use the functions\n",
    "roots = select_root_classes()\n",
    "edges = select_direct_possible_edges('PrimaryCellSpecimen')\n",
    "parents = select_direct_parents('BiopsySpecimen')\n",
    "children = select_direct_children('BiopsySpecimen')\n",
    "\n",
    "print('Roots:', roots)\n",
    "print('Edges:', edges)\n",
    "print('Parents:', parents)\n",
    "print('Children:', children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyTree:\n",
    "    def __init__(self):\n",
    "        self.tree = {}\n",
    "\n",
    "    def add_node(self, node, edges, children):\n",
    "        if node not in self.tree:\n",
    "            self.tree[node] = {'edges': edges, 'children': children}\n",
    "\n",
    "    def add_child(self, parent, child):\n",
    "        if parent in self.tree:\n",
    "            self.tree[parent]['children'].append(child)\n",
    "        else:\n",
    "            self.tree[parent] = {'edges': [], 'children': [child]}\n",
    "\n",
    "def aggregate_edges(node, tree):\n",
    "    edges = set(tree[node]['edges'])\n",
    "    queue = [parent_node for parent_node in tree if node in tree[parent_node]['children']]\n",
    "    while queue:\n",
    "        parent_node = queue.pop(0)\n",
    "        edges |= set(tree[parent_node]['edges'])\n",
    "        queue += [grand_parent for grand_parent in tree if parent_node in tree[grand_parent]['children']]\n",
    "    return edges\n",
    "\n",
    "def create_aggregated_edges_dict(tree):\n",
    "    agg_edges_dict = {}\n",
    "    for node in tree.keys():\n",
    "        agg_edges_dict[node] = aggregate_edges(node, tree)\n",
    "    return agg_edges_dict\n",
    "\n",
    "def convert_to_full_uri(graph, abbreviated_uri):\n",
    "    \"\"\"Converts a prefixed URI to a full URI using a rdflib graph's namespace manager.\"\"\"\n",
    "    prefix, uri = abbreviated_uri.split(':')\n",
    "    namespace = graph.namespace_manager.store.namespace(prefix)\n",
    "    return namespace + uri\n",
    "\n",
    "def remove_prefix(uri):\n",
    "    if ':' in uri:\n",
    "        return uri.split(':', 1)[1]\n",
    "    return uri\n",
    "\n",
    "def build_tree(root_nodes, ontology_tree):\n",
    "    queue = root_nodes\n",
    "    while queue:\n",
    "        current_node = queue.pop(0)\n",
    "        unprefixed_node = remove_prefix(current_node)\n",
    "        edges = select_direct_possible_edges(unprefixed_node)\n",
    "        children = select_direct_children(unprefixed_node)\n",
    "\n",
    "        ontology_tree.add_node(current_node, edges, children)\n",
    "\n",
    "        for child in children:\n",
    "            queue.append(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = select_root_classes()\n",
    "ontology_tree = OntologyTree()\n",
    "build_tree(roots, ontology_tree)\n",
    "agg_edges_dict = create_aggregated_edges_dict(ontology_tree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_uri = convert_to_full_uri(ontology_g, \"omics:hasCellType\")\n",
    "print(full_uri)  # Outputs: http://[blabla...]/omics:hasCellType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_all_focus_nodes_and_classes(graph):\n",
    "    select_statement = \"\"\"\n",
    "    SELECT ?node ?class\n",
    "    WHERE {\n",
    "        ?node a ?class .\n",
    "    }\n",
    "    \"\"\"\n",
    "    results = graph.query(select_statement)\n",
    "    focus_nodes_classes_dict = {str(result[0]): str(result[1]) for result in results}\n",
    "    return focus_nodes_classes_dict\n",
    "\n",
    "\n",
    "focus_nodes_class_dict = select_all_focus_nodes_and_classes(study_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = focus_nodes_class_dict['http://data.boehringer.com/uuid/TranscriptOmicsSample/sample-EX51-EX51_545']\n",
    "print(x) # this returns http://data.boehringer.com/ontology/omics/TranscriptOmicsSample\n",
    "print(convert_to_prefixed(ontology_g, x)) # should return omics:TranscriptOmicsSample\n",
    "print(agg_edges_dict[convert_to_prefixed(ontology_g, x)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabularize & Join Study + Violations Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace\n",
    "from rdflib.namespace import NamespaceManager\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "\n",
    "def count_violations(violations: Graph):\n",
    "    d_focus_node_d_source_shape_counts = defaultdict(lambda: defaultdict(int))\n",
    "    d_violation_focus_node = defaultdict((str))\n",
    "    d_violation_source_shape = defaultdict((str))\n",
    "    violation_list = []\n",
    "\n",
    "    for s, p, o in tqdm(violations, desc=\"Processing Graph Once\"):\n",
    "        if p == SH.sourceShape:\n",
    "            d_violation_source_shape[str(s)] = str(o)\n",
    "            violation_list += [str(o)]\n",
    "        elif p == SH.focusNode:\n",
    "            d_violation_focus_node[str(s)] = str(o)\n",
    "\n",
    "    # for key in violation_focus_node dict\n",
    "    for key in tqdm(d_violation_focus_node, desc=\"Going Over Violation Instances\"):\n",
    "        # init with 1 or add 1 if already exists\n",
    "        d_focus_node_d_source_shape_counts[d_violation_focus_node[key]][d_violation_source_shape[key]] += 1\n",
    "\n",
    "    violation_list = list(set(violation_list))\n",
    "    return d_focus_node_d_source_shape_counts, violation_list\n",
    "\n",
    "\n",
    "def save_violation_list(violation_list):\n",
    "    with open(VIOLATION_LIST_FILE, 'w') as f:\n",
    "        json.dump(violation_list, f)  \n",
    "\n",
    "\n",
    "def create_study_dataframe(study: Graph, d_focus_node_d_source_shape_counts, focus_nodes_class_dict, agg_edges_dict):\n",
    "    # for convenience\n",
    "    pre = lambda x: convert_to_prefixed(ontology_g, str(x))\n",
    "\n",
    "    # create an empty study_df, fill the violation counts\n",
    "    study_df = pd.DataFrame(d_focus_node_d_source_shape_counts).T.fillna(0)\n",
    "\n",
    "    count_not_skipped, count_skipped, count_allowed, count_not_allowed_exists, count_not_allowed_empty = 0, 0, 0, 0, 0\n",
    "\n",
    "    # iterate all study triples\n",
    "    for s, p, o in tqdm(study, desc=\"Processing study\"):\n",
    "        # if s is not in d_focus_node_d_source_shape_counts, continue as it is not a focus node\n",
    "        if str(s) not in d_focus_node_d_source_shape_counts:\n",
    "            count_skipped += 1\n",
    "            continue\n",
    "        count_not_skipped += 1\n",
    "\n",
    "        # if p is not in study_df.columns, add it and set it to EdgeNotPresent for all rows because so far there has not been a value for it for anything, and future values will overwrite it anyway\n",
    "        if str(p) not in study_df.columns:\n",
    "            study_df[str(p)] = \"EdgeNotPresent\"\n",
    "\n",
    "        # if object is empty throw exception and print details\n",
    "        if str(o) == \"\":\n",
    "            print(\"empty object for \" + str(s) + \" \" + str(p))\n",
    "            raise Exception(\"empty object for \" + str(s) + \" \" + str(p))\n",
    "\n",
    "        # get class of focus node\n",
    "        focus_node_class = focus_nodes_class_dict.get(str(s), None)\n",
    "        assert focus_node_class is not None, f\"The focus_node_class for {str(s)} was None, but expected a value.\"\n",
    "        # get allowed edges\n",
    "        focus_node_edges = agg_edges_dict.get(convert_to_prefixed(ontology_g, focus_node_class), set())\n",
    "        assert len(focus_node_edges) > 0\n",
    "\n",
    "        # if edge is allowed (and object is present otherwise we wouldn't get here), set value study_df.at[str(s), str(p)] = str(o)\n",
    "        if pre(p) in focus_node_edges:\n",
    "            study_df.at[str(s), str(p)] = str(o)\n",
    "            count_allowed += 1\n",
    "        # if edge is not allowed, and there is no object, set value study_df.at[str(s), str(p)] = \"EdgeNotPresent\"\n",
    "        elif pre(p) not in focus_node_edges and str(o) == \"\":\n",
    "            study_df.at[str(s), str(p)] = \"EdgeNotPresent\"\n",
    "            count_not_allowed_empty += 1\n",
    "        # if edge is not allowed, and there is an object, set value study_df.at[str(s), str(p)] = \"InvalidEdgeWUnexpectedEdgeWithDataithData: str(o)\"\n",
    "        elif pre(p) not in focus_node_edges and str(o) != \"\":\n",
    "            # study_df.at[str(s), str(p)] = \"UnexpectedEdgeWithData: \" + str(o)\n",
    "            study_df.at[str(s), str(p)] = str(o)\n",
    "            count_not_allowed_exists += 1\n",
    "        else:\n",
    "            raise Exception(\"should not happen: \" + str(s) + \" \" + str(p) + \" \" + str(o))\n",
    "\n",
    "    print(f\"count_skipped = {count_skipped}\")\n",
    "    print(f\"count_not_skipped = {count_not_skipped}\")\n",
    "    print(f\"count_allowed = {count_allowed}\")\n",
    "    print(f\"count_not_allowed_exists = {count_not_allowed_exists}\")\n",
    "    print(f\"count_not_allowed_empty = {count_not_allowed_empty}\")\n",
    "\n",
    "    assert study_df.isin(['']).sum().sum() == 0\n",
    "    assert study_df.isnull().sum().sum() == 0, f\"study_df.isnull().sum().sum() = {study_df.isnull().sum().sum()}\"\n",
    "\n",
    "    # add focus_node column, set it to the study_df.index, and move it to the very left of the dataframe\n",
    "    study_df['focus_node'] = study_df.index\n",
    "    cols = study_df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]  # move the last column to the first\n",
    "    study_df = study_df[cols]\n",
    "\n",
    "    return study_df\n",
    "\n",
    "\n",
    "def save_study_dataframe(study_df):\n",
    "    study_df.to_csv(STUDY_CSV_FILE)\n",
    "\n",
    "\n",
    "def tabularize_graphs(study: Graph, violations: Graph):\n",
    "    print('study graph triples:', len(study))  # type: ignore\n",
    "    print('violations graph triples:', len(violations))  # type: ignore\n",
    "    d_focus_node_d_source_shape_counts, violation_list = count_violations(violations)\n",
    "    save_violation_list(violation_list)\n",
    "    study_df = create_study_dataframe(study, d_focus_node_d_source_shape_counts, focus_nodes_class_dict, agg_edges_dict)\n",
    "    save_study_dataframe(study_df)\n",
    "    return d_focus_node_d_source_shape_counts\n",
    "\n",
    "\n",
    "d_focus_node_d_source_shape_counts = tabularize_graphs(study_g, violations_g)\n",
    "\n",
    "study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "study_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests for Violation Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from rdflib import Graph, Literal, Namespace, BNode, URIRef\n",
    "\n",
    "SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "\n",
    "class TestCountViolations(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.violations = Graph()\n",
    "\n",
    "    def test_count_violations_node_with_multiple_and_node_with_double(self):\n",
    "        self.focusNode1 = BNode()\n",
    "        self.focusNode2 = BNode()\n",
    "        self.sourceShape1 = Literal('shape1')\n",
    "        self.sourceShape2 = Literal('shape2')\n",
    "        self.violation1 = BNode()\n",
    "        self.violation2 = BNode()\n",
    "        self.violation3 = BNode()\n",
    "        self.violation4 = BNode()\n",
    "\n",
    "        # Add triples to the violations graph\n",
    "        self.violations.add((self.violation1, SH.focusNode, self.focusNode1))\n",
    "        self.violations.add((self.violation1, SH.sourceShape, self.sourceShape1))\n",
    "\n",
    "        self.violations.add((self.violation2, SH.focusNode, self.focusNode2))\n",
    "        self.violations.add((self.violation2, SH.sourceShape, self.sourceShape2))\n",
    "\n",
    "        self.violations.add((self.violation3, SH.focusNode, self.focusNode1))\n",
    "        self.violations.add((self.violation3, SH.sourceShape, self.sourceShape1))\n",
    "\n",
    "        self.violations.add((self.violation4, SH.focusNode, self.focusNode2))\n",
    "        self.violations.add((self.violation4, SH.sourceShape, self.sourceShape1))\n",
    "        violation_counts, violation_list = count_violations(self.violations)\n",
    "        expected_counts = {\n",
    "            str(self.focusNode1): {str(self.sourceShape1): 2},\n",
    "            str(self.focusNode2): {str(self.sourceShape2): 1, str(self.sourceShape1): 1}\n",
    "        }\n",
    "        expected_list = [str(self.sourceShape1), str(self.sourceShape2)] \n",
    "        \n",
    "        self.assertEqual(violation_counts, expected_counts)\n",
    "        self.assertEqual(set(violation_list), set(expected_list))  # Use set to ignore order\n",
    "\n",
    "    def test_count_violations_basic(self):\n",
    "        self.focusNode1 = BNode()\n",
    "        self.focusNode2 = BNode()\n",
    "        self.focusNode3 = BNode()\n",
    "        self.sourceShape1 = Literal('shape1')\n",
    "        self.sourceShape2 = Literal('shape2')\n",
    "        self.sourceShape3 = Literal('shape3')\n",
    "        self.violation1 = BNode()\n",
    "        self.violation2 = BNode()\n",
    "        self.violation3 = BNode()\n",
    "\n",
    "        # Add triples to the violations graph\n",
    "        self.violations.add((self.violation1, SH.focusNode, self.focusNode1))\n",
    "        self.violations.add((self.violation1, SH.sourceShape, self.sourceShape1))\n",
    "\n",
    "        self.violations.add((self.violation2, SH.focusNode, self.focusNode2))\n",
    "        self.violations.add((self.violation2, SH.sourceShape, self.sourceShape2))\n",
    "\n",
    "        self.violations.add((self.violation3, SH.focusNode, self.focusNode3))\n",
    "        self.violations.add((self.violation3, SH.sourceShape, self.sourceShape3))\n",
    "\n",
    "        violation_counts, violation_list = count_violations(self.violations)\n",
    "        expected_counts = {\n",
    "            str(self.focusNode1): {str(self.sourceShape1): 1},\n",
    "            str(self.focusNode2): {str(self.sourceShape2): 1},\n",
    "            str(self.focusNode3): {str(self.sourceShape3): 1}\n",
    "        }\n",
    "        expected_list = [str(self.sourceShape1), str(self.sourceShape2), str(self.sourceShape3)] \n",
    "        \n",
    "        self.assertEqual(violation_counts, expected_counts)\n",
    "        self.assertEqual(set(violation_list), set(expected_list))  # Use set to ignore order\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestCountViolations)\n",
    "unittest.TextTestRunner().run(suite)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbreviation using QNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import split_uri\n",
    "\n",
    "def namespace_in_nsm(nsm, namespace):\n",
    "    \"\"\"\n",
    "    Checks if a given namespace is in the provided NamespaceManager.\n",
    "\n",
    "    Parameters:\n",
    "    nsm (NamespaceManager): the NamespaceManager object to check\n",
    "    namespace (str): the namespace to look for in the NamespaceManager\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the namespace is in the NamespaceManager, False otherwise\n",
    "    \"\"\"\n",
    "    for prefix, ns in nsm.namespaces():\n",
    "        if str(ns) == str(namespace):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_qname(nsm, uri):\n",
    "    \"\"\"\n",
    "    Returns a QName for a given URI if its namespace is in the provided NamespaceManager.\n",
    "    If the namespace isn't found in the NamespaceManager, it returns the original URI.\n",
    "\n",
    "    Parameters:\n",
    "    nsm (NamespaceManager): the NamespaceManager object to use\n",
    "    uri (str): the URI to transform into a QName\n",
    "\n",
    "    Returns:\n",
    "    str: a QName if the namespace of the URI is in the NamespaceManager, the original URI otherwise\n",
    "    \"\"\"\n",
    "    namespace, name = split_uri(uri)\n",
    "    if namespace_in_nsm(nsm, namespace):\n",
    "        try:\n",
    "            prefix, _, _ = nsm.compute_qname(uri)\n",
    "            return f\"{prefix}:{name}\"\n",
    "        except Exception as e:\n",
    "            return uri\n",
    "    else:\n",
    "        return uri\n",
    "\n",
    "def abbreviate_using_namespaces(study_graph: Graph, violations_graph: Graph):\n",
    "    \"\"\"\n",
    "    Replaces URIs in a DataFrame and a list of violations with QNames if their namespaces\n",
    "    are found in two provided RDF graphs. Saves the updated DataFrame and the list back to disk.\n",
    "\n",
    "    Parameters:\n",
    "    study_graph (Graph): the first RDF graph to use for namespaces\n",
    "    violations_graph (Graph): the second RDF graph to use for namespaces\n",
    "    \"\"\"\n",
    "\n",
    "    # load violation_list\n",
    "    with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "        violation_list = json.load(f)\n",
    "\n",
    "    # read study_df tabularized data\n",
    "    study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "\n",
    "    # Create a new graph to combine namespaces\n",
    "    combined_graph = Graph()\n",
    "\n",
    "    # Get namespaces from both graphs and bind them to the combined graph\n",
    "    for prefix, ns_uri in study_graph.namespace_manager.namespaces():\n",
    "        combined_graph.namespace_manager.bind(prefix, ns_uri)\n",
    "\n",
    "    for prefix, ns_uri in violations_graph.namespace_manager.namespaces():\n",
    "        combined_graph.namespace_manager.bind(prefix, ns_uri)\n",
    "\n",
    "    # Now use the NamespaceManager of the combined graph\n",
    "    nsm_combined = NamespaceManager(combined_graph)\n",
    "\n",
    "    # Change column names\n",
    "    for col in study_df.columns:\n",
    "        try:\n",
    "            study_df.rename(columns={col: get_qname(nsm_combined, col)}, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Change indices and cell values\n",
    "    for col in study_df.columns:\n",
    "        for idx in study_df.index:\n",
    "            try:\n",
    "                study_df.loc[idx, col] = get_qname(nsm_combined, study_df.loc[idx, col])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # change index\n",
    "    for idx in study_df.index:\n",
    "        try:\n",
    "            study_df.rename(index={idx: get_qname(nsm_combined, str(idx))}, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # change violation_list\n",
    "    for i in range(len(violation_list)):\n",
    "        try:\n",
    "            violation_list[i] = get_qname(nsm_combined, violation_list[i])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # store tabular graph as csv\n",
    "    study_df.to_csv(STUDY_CSV_FILE)    \n",
    "\n",
    "    # store violation_list as json\n",
    "    with open(VIOLATION_LIST_FILE, 'w') as f:\n",
    "        json.dump(violation_list, f)\n",
    "\n",
    "abbreviate_using_namespaces(study_g, violations_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "    violation_list = json.load(f)\n",
    "\n",
    "print('unique violations ('+str(len(violation_list))+'):\\n')\n",
    "print('\\n'.join([str(v) for v in violation_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "study_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Violation Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def create_embedding():\n",
    "    # read study.csv\n",
    "    study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "    \n",
    "    # read violation_list.json\n",
    "    with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "        violation_columns = json.load(f)\n",
    "    \n",
    "    view_df = study_df[violation_columns]  # view of study dataframe with only violation columns\n",
    "\n",
    "    print(\"Creating UMAP embedding... (this may take some time)\")\n",
    "    reducer = umap.UMAP(n_neighbors=int(np.sqrt(len(view_df))), min_dist=0.1, n_components=2, random_state=0, verbose=True)\n",
    "    result = reducer.fit_transform(view_df)  # 2D projection\n",
    "\n",
    "    # Check if the result is a tuple and unpack accordingly\n",
    "    if isinstance(result, tuple):\n",
    "        embedding, *_ = result\n",
    "    else:\n",
    "        embedding = result\n",
    "\n",
    "    # Convert to dense numpy array if it's a sparse matrix\n",
    "    if isinstance(embedding, coo_matrix):\n",
    "        embedding = embedding.toarray()\n",
    "\n",
    "    # Assuming that the embedding is now a NumPy array, based on the docstring\n",
    "    study_df[\"x\"] = embedding[:, 0]\n",
    "    study_df[\"y\"] = embedding[:, 1]\n",
    "\n",
    "    study_df.to_csv(STUDY_CSV_FILE)\n",
    "    print(\"UMAP embedding created and saved to\", STUDY_CSV_FILE)\n",
    "\n",
    "\n",
    "def plot_embedding():\n",
    "    study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "\n",
    "    plt.scatter(study_df[\"x\"], study_df[\"y\"])\n",
    "    plt.show()\n",
    "\n",
    "create_embedding()\n",
    "plot_embedding()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace URIs with Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from rdflib import URIRef\n",
    "\n",
    "# list of label predicates in ascending order of priority\n",
    "label_predicates = [\n",
    "    # URIRef('http://data.boehringer.com/ontology/omics/hasValue'),\n",
    "    # URIRef('http://data.boehringer.com/ontology/omics/name'),\n",
    "    URIRef('http://www.w3.org/2004/02/skos/core#prefLabel')\n",
    "]\n",
    "\n",
    "# read tabularized graph .csv as a dataframe, and violations list .json as a list\n",
    "study_df = pd.read_csv(STUDY_CSV_FILE, index_col=0)\n",
    "\n",
    "with open(VIOLATION_LIST_FILE, 'r') as f:\n",
    "    violation_columns = json.load(f)\n",
    "\n",
    "# Create a new graph to combine namespaces\n",
    "combined_graph = Graph()\n",
    "\n",
    "# Get namespaces from both graphs and bind them to the combined graph\n",
    "for prefix, ns_uri in study_g.namespace_manager.namespaces():\n",
    "    combined_graph.namespace_manager.bind(prefix, ns_uri)\n",
    "\n",
    "for prefix, ns_uri in violations_g.namespace_manager.namespaces():\n",
    "    combined_graph.namespace_manager.bind(prefix, ns_uri)\n",
    "\n",
    "# Now use the NamespaceManager of the combined graph\n",
    "nsm_combined = NamespaceManager(combined_graph)\n",
    "\n",
    "# create a dictionary of {s: o} pairs for translating source shapes to their labels\n",
    "label_dict = {}\n",
    "for label_predicate in label_predicates:\n",
    "    temp_dict = {str(s): str(o) for s, p, o in study_g.triples((None, label_predicate, None))}\n",
    "    # replace the temp_dict keys with their corresponding QNames\n",
    "    temp_dict = {get_qname(nsm_combined, k): v for k, v in temp_dict.items()}\n",
    "    # Update label_dict with temp_dict, overwriting existing keys\n",
    "    label_dict.update(temp_dict)\n",
    "\n",
    "# replace all column names, indices, and cell values in the dataframe with their corresponding labels\n",
    "study_df.columns = [label_dict.get(col, col) for col in study_df.columns]\n",
    "study_df.index = pd.Index([label_dict.get(idx, idx) for idx in study_df.index])\n",
    "for col in study_df.columns:\n",
    "    study_df[col] = study_df[col].apply(lambda x: label_dict.get(x, x))\n",
    "\n",
    "# replace all dictionary keys and values with their corresponding labels\n",
    "violation_columns = [label_dict.get(x, x) for x in violation_columns]\n",
    "\n",
    "# store dataframe back to .csv and violations list back to .json\n",
    "study_df.to_csv(STUDY_CSV_FILE)\n",
    "with open(VIOLATION_LIST_FILE, 'w') as f:\n",
    "    json.dump(violation_columns, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Union of Ontology and Violation Report Exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef, RDFS\n",
    "\n",
    "def replace_uris_with_labels(data, label_dict, data_type=\"graph\"):\n",
    "    if data_type == \"graph\":\n",
    "        # Iterate through each triple\n",
    "        for s, p, o in list(data.triples((None, None, None))):\n",
    "            new_s = URIRef(label_dict.get(str(s), s))\n",
    "            new_o = URIRef(label_dict.get(str(o), o))\n",
    "            \n",
    "            if new_s != s or new_o != o:\n",
    "                data.remove((s, p, o))\n",
    "                data.add((new_s, p, new_o))\n",
    "                \n",
    "    elif data_type == \"dict\":\n",
    "        new_data = {}\n",
    "        for key, values in data.items():\n",
    "            new_key = label_dict.get(key, key)\n",
    "            \n",
    "            # If the values are in set form\n",
    "            if isinstance(values, set):\n",
    "                new_values = {label_dict.get(val, val) for val in values}\n",
    "                \n",
    "            # If the values are in dictionary form with concatenated string keys\n",
    "            elif isinstance(values, dict):\n",
    "                new_values = {}\n",
    "                for po_str, count in values.items():\n",
    "                    if '__' in po_str:\n",
    "                        p, o = po_str.split('__')\n",
    "                        new_p = label_dict.get(p, p)\n",
    "                        new_o = label_dict.get(o, o)\n",
    "                        new_po_str = f\"{new_p}__{new_o}\"\n",
    "                        new_values[new_po_str] = count\n",
    "                    else:\n",
    "                        new_po_str = label_dict.get(po_str, po_str)\n",
    "                        new_values[new_po_str] = count\n",
    "            \n",
    "            new_data[new_key] = new_values\n",
    "            \n",
    "        return new_data\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def add_labels_to_graph(graph, label_dict):\n",
    "    for uri, label in label_dict.items():\n",
    "        graph.add((URIRef(uri), URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'), Literal(label)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "import json\n",
    "from routers.utils import get_violation_report_exemplars\n",
    "\n",
    "ontology_union_violation_exemplars_g, edge_count_dict, focus_node_exemplar_dict, exemplar_focus_node_dict = get_violation_report_exemplars(ontology_g, violations_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also replace in joined .ttl of ontology and violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the graph\n",
    "ontology_union_violation_exemplars_g = add_labels_to_graph(ontology_union_violation_exemplars_g, label_dict)\n",
    "\n",
    "# For the dictionaries\n",
    "edge_count_dict = replace_uris_with_labels(edge_count_dict, label_dict, data_type=\"dict\")\n",
    "focus_node_exemplar_dict = replace_uris_with_labels(focus_node_exemplar_dict, label_dict, data_type=\"dict\")\n",
    "exemplar_focus_node_dict = replace_uris_with_labels(exemplar_focus_node_dict, label_dict, data_type=\"dict\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_union_violation_exemplars_g.serialize(destination=OMICS_MODEL_UNION_VIOLATION_EXEMPLAR_TTL_PATH, format=\"ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from routers.utils import save_edge_count_json, save_uri_set_of_uris_dict\n",
    "\n",
    "save_edge_count_json(edge_count_dict, EXEMPLAR_EDGE_COUNT_DICT_PATH)\n",
    "save_uri_set_of_uris_dict(focus_node_exemplar_dict, FOCUS_NODE_EXEMPLAR_DICT_PATH)\n",
    "save_uri_set_of_uris_dict(exemplar_focus_node_dict, EXEMPLAR_FOCUS_NODE_DICT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
